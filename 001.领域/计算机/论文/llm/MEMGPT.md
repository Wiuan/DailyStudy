# 1. 背景
##### 1.1. 核心痛点
- 有限固定长度的上下文窗口**无法**处理 **长期对话**（跨周/月的聊天）或 **长文档**（财报/法律文本≈百万token）
- 扩展上下文长度？（X）：  
  - 直接增大窗口 → 计算时间和内存成本**O(n²)增长**（Transformer注意力机制瓶颈）  
  - 递归摘要 → **信息蒸馏失真**（如遗忘关键细节）
##### 1.2. 灵感来源：操作系统的启示
>  **内存分级（主内存+磁盘）** + **虚拟内存分页** 突破物理内存限制 
# 2. MemGPT设计
##### 2.1. 核心架构
![[Pasted image 20250720163911.png]]

| **内存层级**             | **类比OS** | **功能**   |
| -------------------- | -------- | -------- |
| **Recall Storage**   | 内存条(RAM) | 近期历史     |
| **Working Context**  | CPU缓存    | 高频数据     |
| **Archival Storage** | 硬盘       | 永久存档完整记录 |
##### 2.2. 核心技术：动态内存调度
1. **智能分页机制**：  ![[Pasted image 20250720024518.png]]
   - 当上下文将满 → 触发**中断警告**
   - 自主决策：关键信息存**工作内存**，次要历史存**外部存储**
2. **主动检索机制**：  ![[Pasted image 20250720024621.png]]
   - 需早期信息 → 从外部存储**精准召回**

##### 2.3. 控制流和函数链接
- **链式执行**：支持多步检索（如文档分析时的跨页查询）
# 3. 效果验证
##### 3.1. 实验1：长对话代理（一致性+个性化）
**深度记忆检索（DMR）**（？？？为什么3.5 优于4）
![[Pasted image 20250715161547.png]]
**对话开场质量（SIM-H）**
![[Pasted image 20250720034307.png]]

##### 3.2. 实验2：长文档分析&多跳推理
![[Pasted image 20250714162928.png]]
主流LLM上下文窗口能力对比（2024）

**多文档问答**
- 数据集：从 NaturalQuestions-Open 数据集中选择
- 检索器： OpenAI’s text-embedding-ada-002 embeddings
- 默认存储​​：使用 ​PostgreSQL + pgvector​​ 存储所有文档向量
- 索引优化：​HNSW算法​​（亚秒级检索）
- 限制：理论无限检索 vs 实际提前终止
![[Pasted image 20250716212502.png]]

**嵌套键值检索 （KV）**
![[Pasted image 20250720151323.png]]

# 4. 相关技术

##### 4.1. 长上下文LLM
- 稀疏注意力（Sparse Transformers）
- 上下文窗口扩展（Context Window Extension）
- 神经记忆单元（Neural Memory）

将长上下文LLM视为 ​**​主内存（RAM）​**​，在其上构建操作系统式管理。
##### 4.2. 检索增强模型
- FLARE​
- WebGPT
- ​​CoT+检索​

变为内存操作​， ​**​分层检索**
##### 4.3. LLM智能体
- Sims-style 代理​
- WebGPT​
- Chain-of-Thought​

MemGPT 解决**记得什么**问题

# 5. 总结
![[Pasted image 20250720160211.png]]

**开源地址**：https://research.memgpt.ai  
