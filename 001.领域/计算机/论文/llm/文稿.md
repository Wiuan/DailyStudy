这篇论文的核心是介绍MemGPT在解决LLM上下文限制问题。

我先从背景开始介绍，最近的大预言模型是对话式AI的基石，但LLM使用有限的固定长度的上下文窗口没有办法支持长时间对话（比如因为上下文截断，可能不记得我上周发的细节了），或者对长文档进行推理（比如发个财报，他分析起来也是因为截断，可能会丢失不同位置的数据关联）。
一开始想着简单的扩展上下文长度，发现不太行。一个是计算时间和内存成本会大幅增长。一个是注意力不均衡，递归摘要后，有的关键细节会丢失。
然后受到操作系统启发，设计MemGPT。通过在主内存和磁盘之间分页数据，使应用程序能够处理远远超过可用内存的数据集。

这个是核心的架构图。
这部分是展示出来的上下文窗口，分三份
分成系统指令、工作上下文和FIFO队列。系统指令是只读的，类似操作系统的**内核代码**，存放核心操作规则（如函数调用指令）。工作上下文可以读写，相当于**CPU缓存**，支持实时编辑，存放动态存储高频信息（如当前对话关键事实）。FIFO Queue可以读写，滚动存储最近对话记录 ， 队列溢出时由图中Queue Manager控制）会触发**内存警告**。
这部分是外部的存储层。
左边Archival Storage存储完整历史数据（如所有聊天记录/文档）。是**磁盘级存储** → 容量无限但读取慢。
右边`Recall Storage` 缓存待检索的关键数据（如用户近期偏好） 。是**内存加速层** → 提升高频数据访问速度
这是调度的部分。
Function Executor是函数执行器，能动态调度内存，**主动抓取外部数据** ▶️ 类似操作系统的**系统调用**。这下面有两个例子。
一个是当上下文将满 → 触发**中断警告**。能够自主决策：关键信息存**工作内存**，次要历史存**外部存储**。
再一个是需要这上面队列没有的消息的时候，先查Recall Storage，再查Archival Storage。查到了就”载入Working Context → LLM生成准确回复
再回到架构图。
Queue Manager（队列管理器）： ▶️ 监控FIFO队列容量 → 溢出时**自动转移旧数据**到Recall Storage ▶️ 相当于**内存分页调度器**

这个流程呢，是**用户输入** → 进入`FIFO Queue` 2. **队列将满** → `Queue Manager`触发溢出处理： - 移出旧消息 → 压缩存入`Recall Storage` - 生成**摘要**保留在队列头部（防信息丢失） 3. **需要历史数据** → `Function Executor`执行： - 从`Archival Storage`检索 → 加载到`Working Context` 4. **LLM处理**： - 结合`System Instructions`规则 + `Working Context`实时数据 → 生成输出 5. **结果返回**： - 最终响应写入`Output Buffer` → 返回用户。
图中​**​蓝色箭头​**​（Recall到Function）是高频路径 → 印证近期数据优先原则
绿色箭头从 ​**​LLM Processor → Function Executor​**​ 表示链式调用
从Function Executor​**​返回Working Context的箭头​**​表示链式调用时的数据回流。通过此循环箭头链，MemGPT 实现 ​**​多步推理不中断​**​ → 突破传统LLM单步限制
这就是核心的设计原理。

往下是对比MemGPT的效果。
主要通过两个实验，一个是**长对话管理**，比较MemGPT带来的一致性和个性化的提升。
    针对一次性是引入了一个基于 MSC 数据集的 “深度记忆检索” （DMR） 任务：具体是提出一个问题，这个问题明确引用之前的对话，并且给定一个小范围的预期答案，只能从之前的对话中来回答。
    使用了ROUGE-L 分数（Lin，2004 年）和“LLM 法官”评估生成的响应的质量。
模型生成答案通常包含​**​多余解释​**​，而标准答案极简。这个ROUGE-L Recall (R)是只关注标准答案中的关键词，会过滤冗余描述。
GPT-4呢可以弥补ROUGE对语义一致性的不足。
在这个任务中，GPT-4基础版准确率仅32.1%，加装MemGPT后飙升至92.5%，提升近三倍。 GPT-4 Turbo版搭配MemGPT达到93.4%。​**​MemGPT使任何基础模型的准确率翻倍以上​**

针对个性化是引入评估对话开场白的“吸引力”。
这个表是结果，第一行拿人类写的开场白，作为基准。 SIM-1/3是开场白和给定的历史知识的匹配度。SIM-H是开场白与人类撰写开场白的相似度。前面两个0.8是因为人可能不会完全按照标签生成，会有自然的表达。1是就就是和自己比。
下面GPT-4在SIM-1和SIM-3上得分最高，最会精准抓知识点 。
GPT-3.5 Turbo在SIM-H上得分最高，可见最会说“人话”，开场白最自然 。​


实验2是**长文档分析**。
这个表是2024年主流LLM上下文窗口长度的数据。GPT-4Turbo有128K的上下文窗口，但实际上很多文档都容易超过这些长度。所以需要MemGPT，下面是两个相关的任务

。一个是**多文档问答**，从 NaturalQuestions-Open 数据集中选择一个问题，检索器 根据 OpenAI 的 text-embedding-ada-002 生成向量，按 ​**​余弦相似度​**​ 返回 Top K 相关文档。
用的用 MemGPT 的默认存储设置，它使用 PostgreSQL 进行存档内存存储，并通过 pgvector 扩展启用向量搜索，使用 HNSW 索引来实现大约的亚秒级查询时间

看这个折线图，没有加MemGPT的是直接向LLM塞入文档。因为文档截断会导致信息丢失  ，K增大到某个数据后性能下降。MemGPT是动态分页的，就不受这个K的限制。加了MemGPT的GPT3.5比4的要低，是因为虽然理论上可以通过分页找到足够的检索器调用，但GPT3.5的函数调用不稳定，会比4的要提前停止检索。

另一个任务是**嵌套键值检索**，用来测试模型的多跳推理能力。这个实验把数据总量控制在8K token内（相当于GPT-4的完整上下文长度），排除掉上下文长度不足的影响。这个表是结果，横坐标是链式查询的嵌套层数，不嵌套的话，GPT-3.5/GPT-4表现都还不错（准确率>90%）。嵌套层数增加后，传统LLM的表现就不太行，性能​立即断崖式下跌。加上MemGPT后表现都比不加好。最好的是MemGPT+​**​GPT-4​**不受嵌套级别数量的影响。
​MemGPT 加GPT-4 Turbo 和 GPT-3.5 的 ​**​函数调用能力不如 GPT-4 (基础版) 稳定可靠​**​，所以“调用函数次数不够”，导致嵌套层数增加分数降低。

下面是 MemGPT 和一些现有技术的关系。主要提到了三大类相关工作。长上下文LLM、检索增强模型和LLM智能体。
之前提高LLM上下文长度的技术方向有稀疏注意力，通过减少注意力计算量。上下文窗口扩展，通过微调模型支持更长上下文，和神经记忆单元，通过增加外部记忆模块。MemGPT在这些改进之上，将长上下文LLM视为 ​**​主内存（RAM）​**​，在其上构建操作系统式管理。

检索增强模型的代表工作有FLARE，是允许LLM主动触发检索​，WebGPT，是分页浏览网页，CoT+检索​是检索与推理交替。
MemGPT将检索转化为​**​内存操作**，支持无限次检索，可以多步骤推理。

LLM智能体的代表研究
有Sims-style 代理​，方向是多代理社交模拟。
- WebGPT ​方向是先搜索再回答
- Chain-of-Thought​的是通过中间步骤分解问题
MemGPT **将操作系统设计原则** （内存分层、中断管理）引入大模型，解决长期记忆和上下文扩展问题

**最后总结**这个MemGPT的颠覆性的点在于借鉴操作系统的内存管理机制，

采用主上下文加上外部存储的 分层存储，

通过函数调用触发控制流切换实现 中断机制 

不同于单次请求-响应的传统LLM，是事件驱动推理的进程调度。**所有外部输入转化为事件​**​ → 由内存管理器统一调度 → ​**​突破纯文本交互局限​**，可以与存储介质持续交互。

在长对话和文档分析两个场景验证效果突出。
![[Pasted image 20250720171828.png]]